---
title: "ps5"
date: "2024-11-06"
format: 
  html: 
    code-overlap: wrap
    code-block-font-size: 0.8em
execute:
  eval: true
  echo: true
---

## Submission Steps (10 pts)

1. **This problem set is a paired problem set.**
2. **Determine Partner Roles**
   - Play paper, scissors, rock to decide who is Partner 1.
     - **Partner 1 (name and cnet ID)**: Yue Wang, yuew3
     - **Partner 2 (name and cnet ID)**: Zhuohao Yang, zhuohao
3. **Link Sharing**
   - Partner 1 will accept the problem set and share the link with Partner 2.
   - Note: The link can only be shared once, and cannot be changed after acceptance.
4. **Integrity Statement**
   - “This submission is our work alone and complies with the 30538 integrity policy.”
     - Initials of Partner 1: **“This submission is our work alone and complies with the 30538 integrity policy.”**
     - Initials of Partner 2: **“This submission is our work alone and complies with the 30538 integrity policy.”**
5. **Collaboration Disclosure**
   - “I have uploaded the names of anyone else, other than my partner, I worked with on the problem set here.”
   - [Provide details if applicable]
6. **Late Coins**
   - Late coins used for this pset: **[Number of late coins used]**
   - Late coins left after submission: **[Number of late coins remaining]**
7. **Knit to PDF**
   - Knit `ps5.qmd` to create `ps5.pdf`.
   - Ensure that the PDF does not exceed 25 pages. Use `head()` and re-size figures as needed.
8. **GitHub Submission**
   - **Partner 1**: Push `ps5.qmd` and `ps5.pdf` to your GitHub repository.
9. **Gradescope Submission**
   - **Partner 1**: Submit `ps5.pdf` on Gradescope and add your partner as a collaborator.
10. **Tagging**
    - **Partner 1**: Tag the submission in Gradescope.


```{python}
#Set up
import pandas as pd
import altair as alt
import time
import requests
from bs4 import BeautifulSoup
import lxml
import re
import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```

# Step 1: Develop initial scraper and crawler
## Q1

```{python}
### URL 
url = 'https://oig.hhs.gov/fraud/enforcement/'

# Fetch the webpage content
response = requests.get(url)
html_content = response.text

# Use BeautifulSoup to parse the HTML
soup = BeautifulSoup(html_content, 'html.parser')

### Scripting Title of the enforcement action
# Find all elements that are headings (h1 to h6)
headings = soup.find_all(re.compile('^h[1-6]$'))

# Create a list to hold all extracted headings
extracted_headings = []
for heading in headings:
    extracted_headings.append((heading.name, heading.get_text(strip=True)))

# Print out all headings found
for tag, text in extracted_headings:
    print(f'{tag}: {text}')


### Scripting Date
def scrape_dates():
    # URL of the page you want to scrape
    url = 'https://oig.hhs.gov/fraud/enforcement/'
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # Find all span tags that contain dates
    date_spans = soup.find_all('span', class_='text-base-dark padding-right-105')
    
    # Extract the text from each span tag (the dates)
    dates = [span.get_text(strip=True) for span in date_spans]
    
    # Create a DataFrame to store the collected dates
    df = pd.DataFrame(dates, columns=['Date'])

    return df

# Run the function and print the DataFrame
df_dates = scrape_dates()
print(df_dates)


def scrape_checkbox_data():
    # URL
    url = 'https://oig.hhs.gov/fraud/enforcement/'
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # Extract checkbox related data
    checkboxes = soup.find_all('input', type='checkbox')
    data = [{
        'Type': box.find_next_sibling('label').get_text(strip=True) if box.find_next_sibling('label') else 'No Label',
        'Link': box.get('value', 'No Link Provided')
    } for box in checkboxes]
    
    # Return as DataFrame
    return pd.DataFrame(data)

# Run the function and print the DataFrame
df_checkboxes = scrape_checkbox_data()
print(df_checkboxes)

def scrape_hyperlinks(url):
    # Sending a request to the webpage
    response = requests.get(url)
    if response.status_code != 200:
        return pd.DataFrame()  # Return an empty DataFrame if there's an error loading the page
    
    # Parsing the HTML content
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # Find all <a> tags
    links = soup.find_all('a')
    
    # Extract the href attribute and text from each <a> tag
    data = [{
        'Text': link.get_text(strip=True),
        'URL': link.get('href')
    } for link in links if link.get('href')]

    # Create a DataFrame to store the collected data
    return pd.DataFrame(data)

# Example usage
base_url = 'https://example.com'  # Replace with the actual URL
df_links = scrape_hyperlinks(base_url)
print(df_links)

```




## Q2
```{python}

```


# Step 2: Making the scraper dynamic
## Q1 
### a
```{python}

```


### b
```{python}


```

### c
```{python}


```

# Step 3: Plot data based on scraped data (using altair)
## Q1
```{python}


```

## Q2
```{python}

```

# Step 4: Create maps of enforcement activity
## Q1
```{python}


```

## Q2
```{python}

```

# Extra credit: Calculate the enforcement actions on a per-capita basis
## Q1
```{python}


```

## Q2
```{python}

```

## Q3
```{python}

```
