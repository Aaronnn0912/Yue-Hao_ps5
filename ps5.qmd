---
title: "ps5"
date: "2024-11-06"
format: 
  html: 
    code-overlap: wrap
    code-block-font-size: 0.8em
execute:
  eval: true
  echo: true
---

## Submission Steps (10 pts)

1. **This problem set is a paired problem set.**
2. **Determine Partner Roles**
   - Play paper, scissors, rock to decide who is Partner 1.
     - **Partner 1 (name and cnet ID)**: Yue Wang, yuew3
     - **Partner 2 (name and cnet ID)**: Zhuohao Yang, zhuohao
3. **Link Sharing**
   - Partner 1 will accept the problem set and share the link with Partner 2.
   - Note: The link can only be shared once, and cannot be changed after acceptance.
4. **Integrity Statement**
   - “This submission is our work alone and complies with the 30538 integrity policy.”
     - Initials of Partner 1: **“This submission is our work alone and complies with the 30538 integrity policy.”**
     - Initials of Partner 2: **“This submission is our work alone and complies with the 30538 integrity policy.”**
5. **Collaboration Disclosure**
   - “I have uploaded the names of anyone else, other than my partner, I worked with on the problem set here.”
   - [Provide details if applicable]
6. **Late Coins**
   - Late coins used for this pset: **[Number of late coins used]**
   - Late coins left after submission: **[Number of late coins remaining]**
7. **Knit to PDF**
   - Knit `ps5.qmd` to create `ps5.pdf`.
   - Ensure that the PDF does not exceed 25 pages. Use `head()` and re-size figures as needed.
8. **GitHub Submission**
   - **Partner 1**: Push `ps5.qmd` and `ps5.pdf` to your GitHub repository.
9. **Gradescope Submission**
   - **Partner 1**: Submit `ps5.pdf` on Gradescope and add your partner as a collaborator.
10. **Tagging**
    - **Partner 1**: Tag the submission in Gradescope.


```{python}
#Set up
import pandas as pd
import altair as alt
import time
import requests
from bs4 import BeautifulSoup
import lxml
import re
import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```

# Step 1: Develop initial scraper and crawler
## Q1
```{python}
# Initialize lists to store data
categories = []
titles = []
dates = []
links = []

# Set the target webpage URL
base_url = 'https://oig.hhs.gov/fraud/enforcement/'  # Base URL for enforcement actions

# Send an HTTP request to the base URL
response = requests.get(base_url)
response.raise_for_status()  # Ensure the request is successful

# Parse the HTML content
soup = BeautifulSoup(response.text, 'html.parser')

# Find all enforcement action cards
entries = soup.select('li.usa-card')

for entry in entries:
    title_anchor = entry.select_one('h2.usa-card__heading a')
    if not title_anchor:
        print("Debug: Title anchor not found. Here's the entry HTML:")
        print(entry)  # Show the HTML content of the entry if the title anchor isn't found.
        continue  # Skip to the next entry if no title anchor is found.

    title = title_anchor.text.strip() if title_anchor else 'No Title Listed'
    relative_link = title_anchor['href'] if title_anchor else 'No Link Available'
    full_link = requests.compat.urljoin(base_url, relative_link)  # Create full URL from base and relative link
    date = entry.select_one('span.text-base-dark.padding-right-105').text.strip() if entry.select_one('span.text-base-dark.padding-right-105') else 'No Date Listed'
    category_list = [category.text.strip() for category in entry.select('ul.display-inline.add-list-reset li')]

    # Append each item to its respective list
    titles.append(title)
    links.append(full_link)
    dates.append(date)
    categories.append(', '.join(category_list))  # Join categories to store as a single string per entry

    # Print debug info
    print("Title:", title)
    print("Link:", full_link)
    print("Date:", date)
    print("Category:", ', '.join(category_list))
    print("---")

# Create a DataFrame
ec = pd.DataFrame({
    'Title': titles,
    'Link': links,
    'Date': dates,
    'Category': categories
})

# Save the DataFrame to CSV
ec.to_csv('/Users/yuewang1/Desktop/enforcement_actions.csv', index=False)
```

## Q2
```{python}
# Set the target webpage URL
base_url = 'https://oig.hhs.gov/fraud/enforcement/'

# Send an HTTP request to the base URL
response = requests.get(base_url)
response.raise_for_status()  # Ensure the request is successful

# Parse the HTML content
soup = BeautifulSoup(response.text, 'html.parser')

# Initialize a list to store the data
data = []

# Find all enforcement action cards
entries = soup.select('li.usa-card')

for entry in entries:
    # Extract the title and link
    title_anchor = entry.select_one('h2.usa-card__heading a')
    if not title_anchor:
        continue  # Skip this entry if no title anchor is found
    title = title_anchor.text.strip()
    link = requests.compat.urljoin(base_url, title_anchor['href'])

    # Extract the date
    date = entry.select_one('span.text-base-dark.padding-right-105').text.strip() if entry.select_one('span.text-base-dark.padding-right-105') else 'Date not listed'

    # Extract the categories
    category_list = [category.text.strip() for category in entry.select('ul.display-inline.add-list-reset li')]
    category_text = ', '.join(category_list)

    # Fetch the detail page to get the agency name
    detail_response = requests.get(link)
    detail_response.raise_for_status()
    detail_soup = BeautifulSoup(detail_response.text, 'html.parser')

    # Extract the agency name
    agency_tag = detail_soup.find(lambda tag: tag.name == 'li' and 'Agency:' in tag.text)
    agency_name = agency_tag.text.split('Agency:')[-1].strip() if agency_tag else 'Agency not listed'

    # Add the data to the list
    data.append({
        'Title': title,
        'Link': link,
        'Date': date,
        'Category': category_text,
        'Agency': agency_name
    })

# Create a DataFrame
df = pd.DataFrame(data)

# Save to CSV file
csv_path = '/Users/yuewang1/Desktop/enforcement_actions_details.csv'
df.to_csv(csv_path, index=False, encoding='utf_8_sig')

print(f"Data has been successfully saved to {csv_path}")

```


# Step 2: Making the scraper dynamic
## Q1 
### a
```{python}

```


### b
```{python}


```

### c
```{python}


```

# Step 3: Plot data based on scraped data (using altair)
## Q1
```{python}


```

## Q2
```{python}

```

# Step 4: Create maps of enforcement activity
## Q1
```{python}


```

## Q2
```{python}

```

# Extra credit: Calculate the enforcement actions on a per-capita basis
## Q1
```{python}


```

## Q2
```{python}

```

## Q3
```{python}

```
